commit e43574e61b4f0cab94683d9c94fd31e2ef03f387
Author: Prathmesh Savale <prathmesh.savale@gmail.com>
Date:   Sat Mar 2 13:16:23 2019 +0530

    Don't use global np.random.seed in tests (#13356)
    
    * initial commit
    
    * used random class
    
    * fixed failing testcases, reverted __init__.py
    
    * fixed failing testcases #2
    - passed rng as parameter to ParameterSampler class
    - changed seed from 0 to 42 (as original)
    
    * fixed failing testcases #2
    - passed rng as parameter to SparseRandomProjection class
    
    * fixed failing testcases #4
    - passed rng as parameter to GaussianRandomProjection class
    
    * fixed failing test case because of flake 8

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -86,267 +86,267 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None,
                is_saga=False):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
         .. versionadded:: 0.18
            *loss='multinomial'*
 
     alpha : float, optional
         L2 regularization term in the objective function
         ``(0.5 * alpha * || W ||_F^2)``. Defaults to 1.
 
     beta : float, optional
         L1 regularization term in the objective function
         ``(beta * || W ||_1)``. Only applied if ``is_saga`` is set to True.
         Defaults to 0.
 
     max_iter : int, optional
         The max number of passes over the training data if the stopping
         criteria is not reached. Defaults to 1000.
 
     tol : double, optional
         The stopping criteria for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose : integer, optional
         The verbosity level.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
         the data.  If int, random_state is the seed used by the random number
         generator; If RandomState instance, random_state is the random number
         generator; If None, the random number generator is the RandomState
         instance used by `np.random`.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem : dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
     is_saga : boolean, optional
         Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves
         better in the first epochs, and allow for l1 regularisation.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
-    >>> np.random.seed(0)
-    >>> X = np.random.randn(n_samples, n_features)
-    >>> y = np.random.randn(n_samples)
+    >>> rng = np.random.RandomState(0)
+    >>> X = rng.randn(n_samples, n_features)
+    >>> y = rng.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(
     ...     solver='sag', multi_class='multinomial')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100,
         multi_class='multinomial', n_jobs=None, penalty='l2',
         random_state=None, solver='sag', tol=0.0001, verbose=0,
         warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
 
     Defazio, A., Bach F. & Lacoste-Julien S. (2014).
     SAGA: A Fast Incremental Gradient Method With Support
     for Non-Strongly Convex Composite Objectives
     https://arxiv.org/abs/1407.0202
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         _dtype = [np.float64, np.float32]
         X = check_array(X, dtype=_dtype, accept_sparse='csr', order='C')
         y = check_array(y, dtype=_dtype, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
     beta_scaled = float(beta) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=X.dtype, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
         coef_init = np.zeros((n_features, n_classes), dtype=X.dtype,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
         intercept_init = np.zeros(n_classes, dtype=X.dtype)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient = np.zeros(n_classes, dtype=X.dtype)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
                                         dtype=X.dtype, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
                                      dtype=X.dtype, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept, n_samples=n_samples,
                                    is_saga=is_saga)
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     sag = sag64 if X.dtype == np.float64 else sag32
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
                             beta_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
                             is_saga,
                             verbose)
 
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit f02ef9f52f81c2d212f428092ad7c3f2f3fbd0f5
Author: Joan Massich <mailsik@gmail.com>
Date:   Wed Feb 27 11:14:29 2019 +0100

    LogisticRegression convert to float64 (for SAG solver) (#13243)
    
    * Remove unused code
    
    * Squash all the PR 9040 commits
    
    initial PR commit
    
    seq_dataset.pyx generated from template
    
    seq_dataset.pyx generated from template #2
    
    rename variables
    
    fused types consistency test for seq_dataset
    
    a
    
    sklearn/utils/tests/test_seq_dataset.py
    
    new if statement
    
    add doc
    
    sklearn/utils/seq_dataset.pyx.tp
    
    minor changes
    
    minor changes
    
    typo fix
    
    check numeric accuracy only up 5th decimal
    
    Address oliver's request for changing test name
    
    add test for make_dataset and rename a variable in test_seq_dataset
    
    * FIX tests
    
    * TST more numerically stable test_sgd.test_tol_parameter
    
    * Added benchmarks to compare SAGA 32b and 64b
    
    * Fixing gael's comments
    
    * fix
    
    * solve some issues
    
    * PEP8
    
    * Address lesteve comments
    
    * fix merging
    
    * avoid using assert_equal
    
    * use all_close
    
    * use explicit ArrayDataset64 and CSRDataset64
    
    * fix: remove unused import
    
    * Use parametrized to cover ArrayDaset-CSRDataset-32-64 matrix
    
    * for consistency use 32 first then 64 + add 64 suffix to variables
    
    * it would be cool if this worked !!!
    
    * more verbose version
    
    * revert SGD changes as much as possible.
    
    * Add solvers back to bench_saga
    
    * make 64 explicit in the naming
    
    * remove checking native python type + add comparison between 32 64
    
    * Add whatsnew with everyone with commits
    
    * simplify a bit the testing
    
    * simplify the parametrize
    
    * update whatsnew
    
    * fix pep8

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -86,264 +86,267 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None,
                is_saga=False):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
         .. versionadded:: 0.18
            *loss='multinomial'*
 
     alpha : float, optional
         L2 regularization term in the objective function
         ``(0.5 * alpha * || W ||_F^2)``. Defaults to 1.
 
     beta : float, optional
         L1 regularization term in the objective function
         ``(beta * || W ||_1)``. Only applied if ``is_saga`` is set to True.
         Defaults to 0.
 
     max_iter : int, optional
         The max number of passes over the training data if the stopping
         criteria is not reached. Defaults to 1000.
 
     tol : double, optional
         The stopping criteria for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose : integer, optional
         The verbosity level.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
         the data.  If int, random_state is the seed used by the random number
         generator; If RandomState instance, random_state is the random number
         generator; If None, the random number generator is the RandomState
         instance used by `np.random`.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem : dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
     is_saga : boolean, optional
         Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves
         better in the first epochs, and allow for l1 regularisation.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(
     ...     solver='sag', multi_class='multinomial')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100,
         multi_class='multinomial', n_jobs=None, penalty='l2',
         random_state=None, solver='sag', tol=0.0001, verbose=0,
         warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
 
     Defazio, A., Bach F. & Lacoste-Julien S. (2014).
     SAGA: A Fast Incremental Gradient Method With Support
     for Non-Strongly Convex Composite Objectives
     https://arxiv.org/abs/1407.0202
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
-        X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
-        y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
+        _dtype = [np.float64, np.float32]
+        X = check_array(X, dtype=_dtype, accept_sparse='csr', order='C')
+        y = check_array(y, dtype=_dtype, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
     beta_scaled = float(beta) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
-        sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
+        sample_weight = np.ones(n_samples, dtype=X.dtype, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
-        coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
+        coef_init = np.zeros((n_features, n_classes), dtype=X.dtype,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
-        intercept_init = np.zeros(n_classes, dtype=np.float64)
+        intercept_init = np.zeros(n_classes, dtype=X.dtype)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
-        intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
+        intercept_sum_gradient = np.zeros(n_classes, dtype=X.dtype)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
-                                        dtype=np.float64, order='C')
+                                        dtype=X.dtype, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
-                                     dtype=np.float64, order='C')
+                                     dtype=X.dtype, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept, n_samples=n_samples,
                                    is_saga=is_saga)
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
+    sag = sag64 if X.dtype == np.float64 else sag32
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
                             beta_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
                             is_saga,
                             verbose)
+
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit c1f58745be7c4923cf0a666f1c6bf052042f131e
Author: Nicolas Hug <contact@nicolas-hug.com>
Date:   Wed Nov 21 20:23:57 2018 -0500

    [MRG] Add elastic net penalty to LogisticRegression (#11646)
    
    * First draft on elasticnet penaly for LogisticRegression
    
    * Some basic tests
    
    * Doc update
    
    * First draft for LogisticRegressionCV.
    
    It seems to be working for binary classification and for multiclass when
    multi_class='ovr'. I'm having a hard time figuring out the intricacies
    of multi_class='multinomial'.
    
    * Changed default to None for l1_ratio.
    
    added warning message is user sets l1_ratio while penalty is not
    elastic-net
    
    * Some more doc
    
    * Updated example to plot elastic net sparsity
    
    * Fixed flake8
    
    * Fixed test by not modifying attribute in fit
    
    * Fixed doc issues
    
    * WIP
    
    * Partially fixed logistic_reg_CV for multinomial.
    
    Also added some comments that are hopefully clear.
    Still need to fix refit=False
    
    * Fixed doc issue
    
    * WIP
    
    * Fixed test for refit=False in LogisticRegressionCV
    
    * Fixed Python 2 numpy version issue
    
    * minor doc updates
    
    * Weird doc error...
    
    * Added test to ensure that elastic net is at least as good as L1 or L2
    once l1_ratio has been optimized with grid search
    
    Also addressed minor reviews
    
    * Fixed test
    
    * addressed comments
    
    * Added back ignore warning on tests
    
    * Added a functional test
    
    * Scale data in test... Now failing
    
    * elastic-net --> elasticnet
    
    * Updated doc for some attributes and checked their shape in tests
    
    * Added l1_ratio dimension to coefs_paths and scores attr
    
    * improve example + fix test
    
    * FIX incorrect lagged_update in SAGA
    
    * Add non-regression test for SAGA's bug
    
    * FIX flake8 and warning
    
    * Re fixed warning
    
    * Updated some tests
    
    * Addressed comments
    
    * more comments and added dimension to LogisticRegressionCV.n_iter_ attribute
    
    * Updated whatsnew for 0.21
    
    * better doc shape looks
    
    * Fixed whatnew entry after merges
    
    * Added dot
    
    * Addressed comments + standardized optional default param docstrings
    
    * Addessed comments
    
    * use swapaxes instead of unsupported moveaxis (hopefully fixes tests)

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -86,264 +86,264 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None,
                is_saga=False):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
         .. versionadded:: 0.18
            *loss='multinomial'*
 
     alpha : float, optional
         L2 regularization term in the objective function
         ``(0.5 * alpha * || W ||_F^2)``. Defaults to 1.
 
     beta : float, optional
         L1 regularization term in the objective function
         ``(beta * || W ||_1)``. Only applied if ``is_saga`` is set to True.
         Defaults to 0.
 
     max_iter : int, optional
         The max number of passes over the training data if the stopping
         criteria is not reached. Defaults to 1000.
 
     tol : double, optional
         The stopping criteria for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose : integer, optional
         The verbosity level.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
         the data.  If int, random_state is the seed used by the random number
         generator; If RandomState instance, random_state is the random number
         generator; If None, the random number generator is the RandomState
         instance used by `np.random`.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem : dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
     is_saga : boolean, optional
         Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves
         better in the first epochs, and allow for l1 regularisation.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(
     ...     solver='sag', multi_class='multinomial')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
-        fit_intercept=True, intercept_scaling=1, max_iter=100,
+        fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100,
         multi_class='multinomial', n_jobs=None, penalty='l2',
         random_state=None, solver='sag', tol=0.0001, verbose=0,
         warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
 
     Defazio, A., Bach F. & Lacoste-Julien S. (2014).
     SAGA: A Fast Incremental Gradient Method With Support
     for Non-Strongly Convex Composite Objectives
     https://arxiv.org/abs/1407.0202
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
     beta_scaled = float(beta) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
         coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
         intercept_init = np.zeros(n_classes, dtype=np.float64)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
                                         dtype=np.float64, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
                                      dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept, n_samples=n_samples,
                                    is_saga=is_saga)
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
                             beta_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
                             is_saga,
                             verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit 7ed61a24feb4ffde0bee9342acf4a58e3f946a61
Author: Joel Nothman <joel.nothman@gmail.com>
Date:   Mon Aug 27 06:00:02 2018 +1000

    ENH add multi_class='auto' for LogisticRegression, default from 0.22; default solver will be 'lbfgs' (#11905)
    
    * Change default solver in LogisticRegression
    * This is an API change, not a feature
    * Decrease numerical precision in LogisticRegression doctest
    * ENH add multi_class='auto' for LR, default from 0.22
    * No warning when binary

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -86,262 +86,264 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None,
                is_saga=False):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
         .. versionadded:: 0.18
            *loss='multinomial'*
 
     alpha : float, optional
         L2 regularization term in the objective function
         ``(0.5 * alpha * || W ||_F^2)``. Defaults to 1.
 
     beta : float, optional
         L1 regularization term in the objective function
         ``(beta * || W ||_1)``. Only applied if ``is_saga`` is set to True.
         Defaults to 0.
 
     max_iter : int, optional
         The max number of passes over the training data if the stopping
         criteria is not reached. Defaults to 1000.
 
     tol : double, optional
         The stopping criteria for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose : integer, optional
         The verbosity level.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
         the data.  If int, random_state is the seed used by the random number
         generator; If RandomState instance, random_state is the random number
         generator; If None, the random number generator is the RandomState
         instance used by `np.random`.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem : dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
     is_saga : boolean, optional
         Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves
         better in the first epochs, and allow for l1 regularisation.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
-    >>> clf = linear_model.LogisticRegression(solver='sag')
+    >>> clf = linear_model.LogisticRegression(
+    ...     solver='sag', multi_class='multinomial')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
-        multi_class='ovr', n_jobs=None, penalty='l2', random_state=None,
-        solver='sag', tol=0.0001, verbose=0, warm_start=False)
+        multi_class='multinomial', n_jobs=None, penalty='l2',
+        random_state=None, solver='sag', tol=0.0001, verbose=0,
+        warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
 
     Defazio, A., Bach F. & Lacoste-Julien S. (2014).
     SAGA: A Fast Incremental Gradient Method With Support
     for Non-Strongly Convex Composite Objectives
     https://arxiv.org/abs/1407.0202
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
     beta_scaled = float(beta) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
         coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
         intercept_init = np.zeros(n_classes, dtype=np.float64)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
                                         dtype=np.float64, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
                                      dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept, n_samples=n_samples,
                                    is_saga=is_saga)
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
                             beta_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
                             is_saga,
                             verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit 40e6c43cb49d6c1eb56faa46ca0ffa40ab6228b8
Author: Olivier Grisel <olivier.grisel@ensta.org>
Date:   Fri Aug 3 12:34:25 2018 +0200

    Joblib 0.12.2 (#11741)
    
    * joblib 0.12.2
    
    * Export _joblib's register_parallel_backend
    
    * Use latest version of coverage

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -86,262 +86,262 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None,
                is_saga=False):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
         .. versionadded:: 0.18
            *loss='multinomial'*
 
     alpha : float, optional
         L2 regularization term in the objective function
         ``(0.5 * alpha * || W ||_F^2)``. Defaults to 1.
 
     beta : float, optional
         L1 regularization term in the objective function
         ``(beta * || W ||_1)``. Only applied if ``is_saga`` is set to True.
         Defaults to 0.
 
     max_iter : int, optional
         The max number of passes over the training data if the stopping
         criteria is not reached. Defaults to 1000.
 
     tol : double, optional
         The stopping criteria for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose : integer, optional
         The verbosity level.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
         the data.  If int, random_state is the seed used by the random number
         generator; If RandomState instance, random_state is the random number
         generator; If None, the random number generator is the RandomState
         instance used by `np.random`.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem : dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
     is_saga : boolean, optional
         Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves
         better in the first epochs, and allow for l1 regularisation.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
-        multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
+        multi_class='ovr', n_jobs=None, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
 
     Defazio, A., Bach F. & Lacoste-Julien S. (2014).
     SAGA: A Fast Incremental Gradient Method With Support
     for Non-Strongly Convex Composite Objectives
     https://arxiv.org/abs/1407.0202
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
     beta_scaled = float(beta) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
         coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
         intercept_init = np.zeros(n_classes, dtype=np.float64)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
                                         dtype=np.float64, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
                                      dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept, n_samples=n_samples,
                                    is_saga=is_saga)
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
                             beta_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
                             is_saga,
                             verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit 9808810028f5358c53c474c223545a0b7ae82b71
Author: Patrick Olden <patrickolden@gmail.com>
Date:   Thu Nov 23 12:04:04 2017 +0000

    [MRG] Added missing sag_solver documentation (#10172)

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -86,256 +86,262 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None,
                is_saga=False):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
         .. versionadded:: 0.18
            *loss='multinomial'*
 
     alpha : float, optional
-        Constant that multiplies the regularization term. Defaults to 1.
+        L2 regularization term in the objective function
+        ``(0.5 * alpha * || W ||_F^2)``. Defaults to 1.
+
+    beta : float, optional
+        L1 regularization term in the objective function
+        ``(beta * || W ||_1)``. Only applied if ``is_saga`` is set to True.
+        Defaults to 0.
 
     max_iter : int, optional
         The max number of passes over the training data if the stopping
         criteria is not reached. Defaults to 1000.
 
     tol : double, optional
         The stopping criteria for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose : integer, optional
         The verbosity level.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
         the data.  If int, random_state is the seed used by the random number
         generator; If RandomState instance, random_state is the random number
         generator; If None, the random number generator is the RandomState
         instance used by `np.random`.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem : dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
     is_saga : boolean, optional
         Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves
         better in the first epochs, and allow for l1 regularisation.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
         multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
 
     Defazio, A., Bach F. & Lacoste-Julien S. (2014).
     SAGA: A Fast Incremental Gradient Method With Support
     for Non-Strongly Convex Composite Objectives
     https://arxiv.org/abs/1407.0202
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
     beta_scaled = float(beta) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
         coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
         intercept_init = np.zeros(n_classes, dtype=np.float64)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
                                         dtype=np.float64, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
                                      dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept, n_samples=n_samples,
                                    is_saga=is_saga)
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
                             beta_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
                             is_saga,
                             verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit e3c9ae204ffb152c151e9b61306ff8f16a2c1e0a
Author: Guillaume Lemaitre <g.lemaitre58@gmail.com>
Date:   Thu Apr 6 02:43:21 2017 +0200

    [MRG+1] DOC improve description and consistency of random_state (#8689)
    
    * DOC improve description of random_state in train_test_split
    
    * DOC Make random_state consistent through documentation
    
    * FIX reverse doc mistake
    
    * FIX address comment of Tom
    
    * DOC address comments
    
    * DOC remove empty line
    
    * DOC remove unecessary white spaces

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -86,253 +86,256 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None,
                is_saga=False):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
         .. versionadded:: 0.18
            *loss='multinomial'*
 
     alpha : float, optional
         Constant that multiplies the regularization term. Defaults to 1.
 
     max_iter : int, optional
         The max number of passes over the training data if the stopping
         criteria is not reached. Defaults to 1000.
 
     tol : double, optional
         The stopping criteria for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose : integer, optional
         The verbosity level.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data.
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem : dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
     is_saga : boolean, optional
         Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves
         better in the first epochs, and allow for l1 regularisation.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
         multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
 
     Defazio, A., Bach F. & Lacoste-Julien S. (2014).
     SAGA: A Fast Incremental Gradient Method With Support
     for Non-Strongly Convex Composite Objectives
     https://arxiv.org/abs/1407.0202
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
     beta_scaled = float(beta) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
         coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
         intercept_init = np.zeros(n_classes, dtype=np.float64)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
                                         dtype=np.float64, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
                                      dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept, n_samples=n_samples,
                                    is_saga=is_saga)
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
                             beta_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
                             is_saga,
                             verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit 5147fd09c6a063188efde444f47bd006fa5f95f0
Author: Arthur Mensch <arthur.mensch@inria.fr>
Date:   Mon Mar 27 21:39:30 2017 +0200

    Add SAGA solver for LogisticRegression and Ridge (#8446)

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -62,240 +86,253 @@
-def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
+def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
-               warm_start_mem=None):
+               warm_start_mem=None,
+               is_saga=False):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
         .. versionadded:: 0.18
            *loss='multinomial'*
 
     alpha : float, optional
         Constant that multiplies the regularization term. Defaults to 1.
 
     max_iter : int, optional
         The max number of passes over the training data if the stopping
         criteria is not reached. Defaults to 1000.
 
     tol : double, optional
         The stopping criteria for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose : integer, optional
         The verbosity level.
 
     random_state : int seed, RandomState instance, or None (default)
         The seed of the pseudo random number generator to use when
         shuffling the data.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem : dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
+    is_saga : boolean, optional
+        Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves
+        better in the first epochs, and allow for l1 regularisation.
+
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
         multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
 
+    Defazio, A., Bach F. & Lacoste-Julien S. (2014).
+    SAGA: A Fast Incremental Gradient Method With Support
+    for Non-Strongly Convex Composite Objectives
+    https://arxiv.org/abs/1407.0202
+
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
+    beta_scaled = float(beta) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
         coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
         intercept_init = np.zeros(n_classes, dtype=np.float64)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
                                         dtype=np.float64, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
                                      dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
-                                   fit_intercept)
-
+                                   fit_intercept, n_samples=n_samples,
+                                   is_saga=is_saga)
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
+                            beta_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
+                            is_saga,
                             verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit 8570622a4450fe1ee3c683601454a7189dcccc14
Author: Joel Nothman <joel.nothman@gmail.com>
Date:   Fri Nov 25 20:59:22 2016 +1100

    [MRG+1] DOC insert spaces before colons in parameter lists (#7920)
    
    * DOC insert spaces before colons in parameter lists
    
    Complies with numpydoc to improve rendering and automatic quality
    assurance such as #7793. Affects listings of Parameters Attributes,
    Returns.
    
    Performed with the help of:
    
        grep -nE '^(    )+[a-zA-Z][a-zA-Z0-9_]*: ' sklearn -R | grep -v -e
        externals -e tests | grep -v -e default: -e else: -e Warning: -e Note:
        -e TRAIN: -e Default: -e True: -e False: -e DOI: -e In: | gsed
        's|\([^:]*\):\([0-9]*\):\([^:]*\):\(.*\)|--- a/\1\n+++ b/\1\n@@ -\2,1
        +\2,1 @@\n-\3:\4\n+\3 :\4|' | git apply --unidiff-zero -
    
    * DOC fix numpydoc format for param

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -62,240 +62,240 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
         .. versionadded:: 0.18
            *loss='multinomial'*
 
     alpha : float, optional
         Constant that multiplies the regularization term. Defaults to 1.
 
-    max_iter: int, optional
+    max_iter : int, optional
         The max number of passes over the training data if the stopping
         criteria is not reached. Defaults to 1000.
 
-    tol: double, optional
+    tol : double, optional
         The stopping criteria for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
-    verbose: integer, optional
+    verbose : integer, optional
         The verbosity level.
 
     random_state : int seed, RandomState instance, or None (default)
         The seed of the pseudo random number generator to use when
         shuffling the data.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
-    warm_start_mem: dict, optional
+    warm_start_mem : dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
         multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
         coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
         intercept_init = np.zeros(n_classes, dtype=np.float64)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
                                         dtype=np.float64, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
                                      dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept)
 
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
                             verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit 32236ff9a52e83e4c6e9984bdefc76082af537d5
Author: Om Prakash <omprakash070@gmail.com>
Date:   Thu Sep 29 16:52:49 2016 +0530

    [MRG+1] minor link fix for SAG paper. (#7523)
    
    * Link to SAG paper updated
    
    link to SAG paper in `References` section updated (#7512).
    
    * Link to SAG paper updated
    
    dead link of SAG paper in Logistic Regression user guide updated.
    resolves #7512
    
    * SAG paper cited in docstring
    
    SAG paper cited in `Reference` section of docstring(#7512).

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -62,240 +62,240 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
         .. versionadded:: 0.18
            *loss='multinomial'*
 
     alpha : float, optional
         Constant that multiplies the regularization term. Defaults to 1.
 
     max_iter: int, optional
         The max number of passes over the training data if the stopping
         criteria is not reached. Defaults to 1000.
 
     tol: double, optional
         The stopping criteria for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose: integer, optional
         The verbosity level.
 
     random_state : int seed, RandomState instance, or None (default)
         The seed of the pseudo random number generator to use when
         shuffling the data.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem: dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
         multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
-    https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
+    https://hal.inria.fr/hal-00860051/document
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
         coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
         intercept_init = np.zeros(n_classes, dtype=np.float64)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
                                         dtype=np.float64, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
                                      dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept)
 
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
                             verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit e2a2b4d403742c3eb2d0e085631a014b0975d6af
Author: Shota <shotat@users.noreply.github.com>
Date:   Tue Jun 28 01:13:49 2016 +0900

    Fix typos (#6942)

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -62,240 +62,240 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
         .. versionadded:: 0.18
            *loss='multinomial'*
 
     alpha : float, optional
         Constant that multiplies the regularization term. Defaults to 1.
 
     max_iter: int, optional
         The max number of passes over the training data if the stopping
-        criterea is not reached. Defaults to 1000.
+        criteria is not reached. Defaults to 1000.
 
     tol: double, optional
-        The stopping criterea for the weights. The iterations will stop when
+        The stopping criteria for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose: integer, optional
         The verbosity level.
 
     random_state : int seed, RandomState instance, or None (default)
         The seed of the pseudo random number generator to use when
         shuffling the data.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem: dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
         multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
         coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
         intercept_init = np.zeros(n_classes, dtype=np.float64)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
                                         dtype=np.float64, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
                                      dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept)
 
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
                             verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit e152df2a62345ff5c9e9305bd5b8b95be74ff21b
Author: Tom Dupré la Tour <tom.dupre-la-tour@m4x.org>
Date:   Fri Dec 11 11:39:10 2015 +0100

    DOC add version_added for multinomial sag

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -62,237 +62,240 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values. With loss='multinomial', y must be label encoded
         (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared' | 'multinomial'
         Loss function that will be optimized:
         -'log' is the binary logistic loss, as used in LogisticRegression.
         -'squared' is the squared loss, as used in Ridge.
         -'multinomial' is the multinomial logistic loss, as used in
          LogisticRegression.
 
+        .. versionadded:: 0.18
+           *loss='multinomial'*
+
     alpha : float, optional
         Constant that multiplies the regularization term. Defaults to 1.
 
     max_iter: int, optional
         The max number of passes over the training data if the stopping
         criterea is not reached. Defaults to 1000.
 
     tol: double, optional
         The stopping criterea for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose: integer, optional
         The verbosity level.
 
     random_state : int seed, RandomState instance, or None (default)
         The seed of the pseudo random number generator to use when
         shuffling the data.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem: dict, optional
         The initialization parameters used for warm starting. Warm starting is
         currently used in LogisticRegression but not in Ridge.
         It contains:
             - 'coef': the weight vector, with the intercept in last line
                 if the intercept is fitted.
             - 'gradient_memory': the scalar gradient for all seen samples.
             - 'sum_gradient': the sum of gradient over all seen samples,
                 for each feature.
             - 'intercept_sum_gradient': the sum of gradient over all seen
                 samples, for the intercept.
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
         multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         # assume fit_intercept is False
         coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
                              order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1, :]
         coef_init = coef_init[:-1, :]
     else:
         intercept_init = np.zeros(n_classes, dtype=np.float64)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros((n_samples, n_classes),
                                         dtype=np.float64, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros((n_features, n_classes),
                                      dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept)
 
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     num_seen, n_iter_ = sag(dataset, coef_init,
                             intercept_init, n_samples,
                             n_features, n_classes, tol,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
                             num_seen_init,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
                             verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     if fit_intercept:
         coef_init = np.vstack((coef_init, intercept_init))
 
     warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     if loss == 'multinomial':
         coef_ = coef_init.T
     else:
         coef_ = coef_init[:, 0]
 
     return coef_, n_iter_, warm_start_mem

commit 9f136ff293656f83ec04aa1b13258d740e2fa1e8
Author: TomDLT <tom.dupre-la-tour@m4x.org>
Date:   Wed Jul 29 15:04:29 2015 +0200

    ENH add multinomial SAG solver for LogisticRegression

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -57,224 +62,237 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     .. versionadded:: 0.17
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
-        Target values
+        Target values. With loss='multinomial', y must be label encoded
+        (see preprocessing.LabelEncoder).
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
-    loss : 'log' | 'squared'
-        Loss function that will be optimized.
-        'log' is used for classification, like in LogisticRegression.
-        'squared' is used for regression, like in Ridge.
+    loss : 'log' | 'squared' | 'multinomial'
+        Loss function that will be optimized:
+        -'log' is the binary logistic loss, as used in LogisticRegression.
+        -'squared' is the squared loss, as used in Ridge.
+        -'multinomial' is the multinomial logistic loss, as used in
+         LogisticRegression.
 
     alpha : float, optional
         Constant that multiplies the regularization term. Defaults to 1.
 
     max_iter: int, optional
         The max number of passes over the training data if the stopping
         criterea is not reached. Defaults to 1000.
 
     tol: double, optional
         The stopping criterea for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose: integer, optional
         The verbosity level.
 
     random_state : int seed, RandomState instance, or None (default)
         The seed of the pseudo random number generator to use when
         shuffling the data.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem: dict, optional
-        The initialization parameters used for warm starting. It is currently
-        not used in Ridge.
+        The initialization parameters used for warm starting. Warm starting is
+        currently used in LogisticRegression but not in Ridge.
+        It contains:
+            - 'coef': the weight vector, with the intercept in last line
+                if the intercept is fitted.
+            - 'gradient_memory': the scalar gradient for all seen samples.
+            - 'sum_gradient': the sum of gradient over all seen samples,
+                for each feature.
+            - 'intercept_sum_gradient': the sum of gradient over all seen
+                samples, for the intercept.
+            - 'seen': array of boolean describing the seen samples.
+            - 'num_seen': the number of seen samples.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
-        Contains a 'coef' key with the fitted result, and eventually the
+        Contains a 'coef' key with the fitted result, and possibly the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
         multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
 
+    # if loss == 'multinomial', y should be label encoded.
+    n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
+
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
-        coef_init = np.zeros(n_features, dtype=np.float64, order='C')
+        # assume fit_intercept is False
+        coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
+                             order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
-    fit_intercept = coef_init.size == (n_features + 1)
+    fit_intercept = coef_init.shape[0] == (n_features + 1)
     if fit_intercept:
-        intercept_init = coef_init[-1]
-        coef_init = coef_init[:-1]
+        intercept_init = coef_init[-1, :]
+        coef_init = coef_init[:-1, :]
     else:
-        intercept_init = 0.0
+        intercept_init = np.zeros(n_classes, dtype=np.float64)
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
-        intercept_sum_gradient_init = warm_start_mem['intercept_sum_gradient']
+        intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
     else:
-        intercept_sum_gradient_init = 0.0
+        intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
-        gradient_memory_init = np.zeros(n_samples, dtype=np.float64,
-                                        order='C')
+        gradient_memory_init = np.zeros((n_samples, n_classes),
+                                        dtype=np.float64, order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
-        sum_gradient_init = np.zeros(n_features, dtype=np.float64, order='C')
+        sum_gradient_init = np.zeros((n_features, n_classes),
+                                     dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
-        max_squared_sum = get_max_squared_sum(X)
+        max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept)
 
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
-    if loss == 'log':
-        class_loss = Log()
-    elif loss == 'squared':
-        class_loss = SquaredLoss()
-    else:
-        raise ValueError("Invalid loss parameter: got %r instead of "
-                         "one of ('log', 'squared')" % loss)
-
-    intercept_, num_seen, n_iter_, intercept_sum_gradient = \
-        sag(dataset, coef_init.ravel(),
-            intercept_init, n_samples,
-            n_features, tol,
-            max_iter,
-            class_loss,
-            step_size, alpha_scaled,
-            sum_gradient_init.ravel(),
-            gradient_memory_init.ravel(),
-            seen_init.ravel(),
-            num_seen_init,
-            fit_intercept,
-            intercept_sum_gradient_init,
-            intercept_decay,
-            verbose)
-
+    num_seen, n_iter_ = sag(dataset, coef_init,
+                            intercept_init, n_samples,
+                            n_features, n_classes, tol,
+                            max_iter,
+                            loss,
+                            step_size, alpha_scaled,
+                            sum_gradient_init,
+                            gradient_memory_init,
+                            seen_init,
+                            num_seen_init,
+                            fit_intercept,
+                            intercept_sum_gradient,
+                            intercept_decay,
+                            verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
-    coef_ = coef_init
     if fit_intercept:
-        coef_ = np.append(coef_, intercept_)
+        coef_init = np.vstack((coef_init, intercept_init))
 
-    warm_start_mem = {'coef': coef_, 'sum_gradient': sum_gradient_init,
+    warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
+    if loss == 'multinomial':
+        coef_ = coef_init.T
+    else:
+        coef_ = coef_init[:, 0]
+
     return coef_, n_iter_, warm_start_mem

commit 2249daaea8ea86b3af22e2624eb7667ea639719d
Author: KamalakerDadi <dkamalakarreddy@gmail.com>
Date:   Wed Oct 21 21:51:04 2015 +0200

    Version added for all new classes

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -57,222 +57,224 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=None):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
+    .. versionadded:: 0.17
+
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared'
         Loss function that will be optimized.
         'log' is used for classification, like in LogisticRegression.
         'squared' is used for regression, like in Ridge.
 
     alpha : float, optional
         Constant that multiplies the regularization term. Defaults to 1.
 
     max_iter: int, optional
         The max number of passes over the training data if the stopping
         criterea is not reached. Defaults to 1000.
 
     tol: double, optional
         The stopping criterea for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose: integer, optional
         The verbosity level.
 
     random_state : int seed, RandomState instance, or None (default)
         The seed of the pseudo random number generator to use when
         shuffling the data.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem: dict, optional
         The initialization parameters used for warm starting. It is currently
         not used in Ridge.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and eventually the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
         multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     if warm_start_mem is None:
         warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         coef_init = np.zeros(n_features, dtype=np.float64, order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.size == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1]
         coef_init = coef_init[:-1]
     else:
         intercept_init = 0.0
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient_init = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient_init = 0.0
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros(n_samples, dtype=np.float64,
                                         order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros(n_features, dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = get_max_squared_sum(X)
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept)
 
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     if loss == 'log':
         class_loss = Log()
     elif loss == 'squared':
         class_loss = SquaredLoss()
     else:
         raise ValueError("Invalid loss parameter: got %r instead of "
                          "one of ('log', 'squared')" % loss)
 
     intercept_, num_seen, n_iter_, intercept_sum_gradient = \
         sag(dataset, coef_init.ravel(),
             intercept_init, n_samples,
             n_features, tol,
             max_iter,
             class_loss,
             step_size, alpha_scaled,
             sum_gradient_init.ravel(),
             gradient_memory_init.ravel(),
             seen_init.ravel(),
             num_seen_init,
             fit_intercept,
             intercept_sum_gradient_init,
             intercept_decay,
             verbose)
 
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     coef_ = coef_init
     if fit_intercept:
         coef_ = np.append(coef_, intercept_)
 
     warm_start_mem = {'coef': coef_, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     return coef_, n_iter_, warm_start_mem

commit c0e816b46fc8017cc4dbee29a8442127f2ab0ca0
Author: Rémy Léone <remy.leone@gmail.com>
Date:   Sat Oct 31 12:19:33 2015 +0100

    Default mutable argument
    
    Default argument values are evaluated only once at function definition
    time, which means that modifying the default value of the argument will
    affect all subsequent calls of the function.

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -57,220 +57,222 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
-               warm_start_mem=dict()):
+               warm_start_mem=None):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared'
         Loss function that will be optimized.
         'log' is used for classification, like in LogisticRegression.
         'squared' is used for regression, like in Ridge.
 
     alpha : float, optional
         Constant that multiplies the regularization term. Defaults to 1.
 
     max_iter: int, optional
         The max number of passes over the training data if the stopping
         criterea is not reached. Defaults to 1000.
 
     tol: double, optional
         The stopping criterea for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose: integer, optional
         The verbosity level.
 
     random_state : int seed, RandomState instance, or None (default)
         The seed of the pseudo random number generator to use when
         shuffling the data.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem: dict, optional
         The initialization parameters used for warm starting. It is currently
         not used in Ridge.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and eventually the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
         multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
+    if warm_start_mem is None:
+        warm_start_mem = {}
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         coef_init = np.zeros(n_features, dtype=np.float64, order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.size == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1]
         coef_init = coef_init[:-1]
     else:
         intercept_init = 0.0
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient_init = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient_init = 0.0
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros(n_samples, dtype=np.float64,
                                         order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros(n_features, dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = get_max_squared_sum(X)
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept)
 
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     if loss == 'log':
         class_loss = Log()
     elif loss == 'squared':
         class_loss = SquaredLoss()
     else:
         raise ValueError("Invalid loss parameter: got %r instead of "
                          "one of ('log', 'squared')" % loss)
 
     intercept_, num_seen, n_iter_, intercept_sum_gradient = \
         sag(dataset, coef_init.ravel(),
             intercept_init, n_samples,
             n_features, tol,
             max_iter,
             class_loss,
             step_size, alpha_scaled,
             sum_gradient_init.ravel(),
             gradient_memory_init.ravel(),
             seen_init.ravel(),
             num_seen_init,
             fit_intercept,
             intercept_sum_gradient_init,
             intercept_decay,
             verbose)
 
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     coef_ = coef_init
     if fit_intercept:
         coef_ = np.append(coef_, intercept_)
 
     warm_start_mem = {'coef': coef_, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     return coef_, n_iter_, warm_start_mem

commit 6dd6f8ffe85798aa8d7495d1485943cf1f93dc59
Author: Andreas Mueller <amueller@nyu.edu>
Date:   Fri Sep 11 17:32:51 2015 -0400

    minor fixes to the doc build

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -57,220 +57,220 @@
 def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
                warm_start_mem=dict()):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
     a constant learning rate.
 
     IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
     same scale. You can normalize the data by using
     sklearn.preprocessing.StandardScaler on your data before passing it to the
     fit method.
 
     This implementation works with data represented as dense numpy arrays or
     sparse scipy arrays of floating point values for the features. It will
     fit the data according to squared loss or log loss.
 
     The regularizer is a penalty added to the loss function that shrinks model
     parameters towards the zero vector using the squared euclidean norm L2.
 
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         Training data
 
     y : numpy array, shape (n_samples,)
         Target values
 
     sample_weight : array-like, shape (n_samples,), optional
         Weights applied to individual samples (1. for unweighted).
 
     loss : 'log' | 'squared'
         Loss function that will be optimized.
         'log' is used for classification, like in LogisticRegression.
         'squared' is used for regression, like in Ridge.
 
     alpha : float, optional
         Constant that multiplies the regularization term. Defaults to 1.
 
     max_iter: int, optional
         The max number of passes over the training data if the stopping
         criterea is not reached. Defaults to 1000.
 
     tol: double, optional
         The stopping criterea for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
     verbose: integer, optional
         The verbosity level.
 
     random_state : int seed, RandomState instance, or None (default)
         The seed of the pseudo random number generator to use when
         shuffling the data.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. If None, it will be computed,
         going through all the samples. The value should be precomputed
         to speed up cross validation.
 
     warm_start_mem: dict, optional
         The initialization parameters used for warm starting. It is currently
         not used in Ridge.
 
     Returns
     -------
     coef_ : array, shape (n_features)
         Weight vector.
 
     n_iter_ : int
         The number of full pass on all samples.
 
     warm_start_mem : dict
         Contains a 'coef' key with the fitted result, and eventually the
         fitted intercept at the end of the array. Contains also other keys
         used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
     >>> X = np.random.randn(n_samples, n_features)
     >>> y = np.random.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='sag', tol=0.001)
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
     >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
         multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
-    Reference
-    ---------
+    References
+    ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
 
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
     LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
     # Ridge default max_iter is None
     if max_iter is None:
         max_iter = 1000
 
     if check_input:
         X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
 
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
 
     # initialization
     if sample_weight is None:
         sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
 
     if 'coef' in warm_start_mem.keys():
         coef_init = warm_start_mem['coef']
     else:
         coef_init = np.zeros(n_features, dtype=np.float64, order='C')
 
     # coef_init contains possibly the intercept_init at the end.
     # Note that Ridge centers the data before fitting, so fit_intercept=False.
     fit_intercept = coef_init.size == (n_features + 1)
     if fit_intercept:
         intercept_init = coef_init[-1]
         coef_init = coef_init[:-1]
     else:
         intercept_init = 0.0
 
     if 'intercept_sum_gradient' in warm_start_mem.keys():
         intercept_sum_gradient_init = warm_start_mem['intercept_sum_gradient']
     else:
         intercept_sum_gradient_init = 0.0
 
     if 'gradient_memory' in warm_start_mem.keys():
         gradient_memory_init = warm_start_mem['gradient_memory']
     else:
         gradient_memory_init = np.zeros(n_samples, dtype=np.float64,
                                         order='C')
     if 'sum_gradient' in warm_start_mem.keys():
         sum_gradient_init = warm_start_mem['sum_gradient']
     else:
         sum_gradient_init = np.zeros(n_features, dtype=np.float64, order='C')
 
     if 'seen' in warm_start_mem.keys():
         seen_init = warm_start_mem['seen']
     else:
         seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
 
     if 'num_seen' in warm_start_mem.keys():
         num_seen_init = warm_start_mem['num_seen']
     else:
         num_seen_init = 0
 
     dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
 
     if max_squared_sum is None:
         max_squared_sum = get_max_squared_sum(X)
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                    fit_intercept)
 
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
 
     if loss == 'log':
         class_loss = Log()
     elif loss == 'squared':
         class_loss = SquaredLoss()
     else:
         raise ValueError("Invalid loss parameter: got %r instead of "
                          "one of ('log', 'squared')" % loss)
 
     intercept_, num_seen, n_iter_, intercept_sum_gradient = \
         sag(dataset, coef_init.ravel(),
             intercept_init, n_samples,
             n_features, tol,
             max_iter,
             class_loss,
             step_size, alpha_scaled,
             sum_gradient_init.ravel(),
             gradient_memory_init.ravel(),
             seen_init.ravel(),
             num_seen_init,
             fit_intercept,
             intercept_sum_gradient_init,
             intercept_decay,
             verbose)
 
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
                       "the coef_ did not converge", ConvergenceWarning)
 
     coef_ = coef_init
     if fit_intercept:
         coef_ = np.append(coef_, intercept_)
 
     warm_start_mem = {'coef': coef_, 'sum_gradient': sum_gradient_init,
                       'intercept_sum_gradient': intercept_sum_gradient,
                       'gradient_memory': gradient_memory_init,
                       'seen': seen_init, 'num_seen': num_seen}
 
     return coef_, n_iter_, warm_start_mem

commit 94eb61960a13f84a8be0dd527012f929172a0d60
Author: TomDLT <tom.dupre-la-tour@m4x.org>
Date:   Tue May 19 10:24:22 2015 +0200

    ENH add sag solver in LogisticRegression and Ridge

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -251,310 +57,220 @@
-    def __init__(self, alpha=0.0001,
-                 fit_intercept=True, max_iter=1000, tol=0.001, verbose=0,
-                 n_jobs=1, random_state=None,
-                 eta0='auto', class_weight=None, warm_start=False):
-        self.n_jobs = n_jobs
-        self.class_weight = class_weight
-        self.loss_function = Log()
-        super(SAGClassifier, self).__init__(alpha=alpha,
-                                            fit_intercept=fit_intercept,
-                                            max_iter=max_iter,
-                                            verbose=verbose,
-                                            random_state=random_state,
-                                            tol=tol,
-                                            eta0=eta0,
-                                            warm_start=warm_start)
-
-    """Fit linear model with Stochastic Average Gradient.
-
-        Parameters
-        ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Training data
-
-        y : numpy array, shape (n_samples,)
-            Target values
-
-        sample_weight : array-like, shape (n_samples,), optional
-            Weights applied to individual samples (1. for unweighted).
-
-        Returns
-        -------
-        self : returns an instance of self.
-        """
-    def fit(self, X, y, sample_weight=None):
-        X, y = check_X_y(X, y, "csr", copy=False, order='C',
-                         dtype=np.float64)
-        n_samples, n_features = X.shape[0], X.shape[1]
-
-        self.classes_ = np.unique(y)
-        self.expanded_class_weight_ = compute_class_weight(self.class_weight,
-                                                           self.classes_, y)
-
-        if self.classes_.shape[0] <= 1:
-            # there is only one class
-            raise ValueError("The number of class labels must be "
-                             "greater than one.")
-        elif self.classes_.shape[0] == 2:
-            # binary classifier
-            (coef, intercept, sum_gradient, gradient_memory,
-             seen, num_seen, intercept_sum_gradient) = \
-                self._fit_target_class(X, y, self.classes_[1], sample_weight)
-        else:
-            # multiclass classifier
-            coef = []
-            intercept = []
-            sum_gradient = []
-            gradient_memory = []
-            seen = []
-            num_seen = []
-            intercept_sum_gradient = []
-
-            # perform a fit for all classes, one verse all
-            results = Parallel(n_jobs=self.n_jobs,
-                               backend="threading",
-                               verbose=self.verbose)(
-                # we have to use a call to multiprocess_method instead of the
-                # plain instance method because pickle will not work on
-                # instance methods in python 2.6 and 2.7
-                delayed(multiprocess_method)(self, "_fit_target_class",
-                                             (X, y, cl, sample_weight))
-                for cl in self.classes_)
-
-            # append results to the correct array
-            for (coef_cl, intercept_cl, sum_gradient_cl, gradient_memory_cl,
-                 seen_cl, num_seen_cl, intercept_sum_gradient_cl) in results:
-                coef.append(coef_cl)
-                intercept.append(intercept_cl)
-                sum_gradient.append(sum_gradient_cl)
-                gradient_memory.append(gradient_memory_cl)
-                seen.append(seen_cl)
-                num_seen.append(num_seen_cl)
-                intercept_sum_gradient.append(intercept_sum_gradient_cl)
-
-            # stack all arrays to transform into np arrays
-            coef = np.vstack(coef)
-            intercept = np.array(intercept)
-            sum_gradient = np.vstack(sum_gradient)
-            gradient_memory = np.vstack(gradient_memory)
-            seen = np.vstack(seen)
-            num_seen = np.array(num_seen)
-            intercept_sum_gradient = np.array(intercept_sum_gradient)
-
-        self.coef_ = coef
-        self.intercept_ = intercept
-        self.sum_gradient_ = sum_gradient
-        self.gradient_memory_ = gradient_memory
-        self.seen_ = seen
-        self.num_seen_ = num_seen
-        self.intercept_sum_gradient_ = intercept_sum_gradient
-
-        return self
-
-    def _fit_target_class(self, X, y, target_class, sample_weight=None):
-        coef_init = None
-        intercept_init = None
-        sum_gradient_init = None
-        gradient_memory_init = None
-        seen_init = None
-        num_seen_init = None
-        intercept_sum_gradient_init = None
-
-        if self.classes_.shape[0] == 2:
-            if self.warm_start:
-                # init parameters for binary classifier
-                coef_init = self.coef_
-                intercept_init = self.intercept_
-                sum_gradient_init = self.sum_gradient_
-                gradient_memory_init = self.gradient_memory_
-                seen_init = self.seen_
-                num_seen_init = self.num_seen_
-                intercept_sum_gradient_init = \
-                    self.intercept_sum_gradient_
-
-            weight_pos = self.expanded_class_weight_[1]
-            weight_neg = self.expanded_class_weight_[0]
-        else:
-            class_index = np.where(self.classes_ == target_class)[0][0]
-            if self.warm_start:
-                # init parameters for multi-class classifier
-                if self.coef_ is not None:
-                    coef_init = self.coef_[class_index]
-                if self.intercept_ is not None:
-                    intercept_init = self.intercept_[class_index]
-                if self.sum_gradient_ is not None:
-                    sum_gradient_init = self.sum_gradient_[class_index]
-                if self.gradient_memory_ is not None:
-                    gradient_memory_init = self.gradient_memory_[class_index]
-                if self.seen_ is not None:
-                    seen_init = self.seen_[class_index]
-                if self.num_seen_ is not None:
-                    num_seen_init = self.num_seen_[class_index]
-                if self.intercept_sum_gradient_ is not None:
-                    intercept_sum_gradient_init = \
-                        self.intercept_sum_gradient_[class_index]
-
-            weight_pos = self.expanded_class_weight_[class_index]
-            weight_neg = 1.0
-
-        n_samples, n_features = X.shape[0], X.shape[1]
-
-        y_encoded = np.ones(n_samples)
-        y_encoded[y != target_class] = -1.0
-
-        return super(SAGClassifier, self).\
-            _fit(X, y_encoded,
-                 coef_init, intercept_init,
-                 sample_weight,
-                 sum_gradient_init,
-                 gradient_memory_init,
-                 seen_init, num_seen_init,
-                 intercept_sum_gradient_init,
-                 weight_pos, weight_neg)
-
-
-class SAGRegressor(BaseSAG, LinearModel, RegressorMixin,
-                   BaseEstimator):
-    """Linear model fitted by minimizing a regularized empirical loss with SAG
+def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
+               max_iter=1000, tol=0.001, verbose=0, random_state=None,
+               check_input=True, max_squared_sum=None,
+               warm_start_mem=dict()):
+    """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
-    a constant learning rate. The inspiration for SAG comes from "Minimizing
-    Finite Sums with the Stochastic Average Gradient" by Mark Schmidt,
-    Nicolas Le Roux, and Francis Bach. 2013. <hal-00860051>
-    https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
+    a constant learning rate.
 
-    IMPORTANT NOTE: SAGRegressor and models from linear_model in general depend
-    on columns that are on the same scale. You can make sure that the data will
-    be normalized by using sklearn.preprocessing.StandardScaler on your data
-    before passing it to the fit method.
+    IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
+    same scale. You can normalize the data by using
+    sklearn.preprocessing.StandardScaler on your data before passing it to the
+    fit method.
 
-    The regularizer is a penalty added to the loss function that shrinks model
-    parameters towards the zero vector using the squared euclidean norm
-    L2.
+    This implementation works with data represented as dense numpy arrays or
+    sparse scipy arrays of floating point values for the features. It will
+    fit the data according to squared loss or log loss.
 
-    This implementation works with data represented as dense or sparse numpy
-    arrays of floating point values for the features.
+    The regularizer is a penalty added to the loss function that shrinks model
+    parameters towards the zero vector using the squared euclidean norm L2.
 
     Parameters
     ----------
-    alpha : float, optional
-        Constant that multiplies the regularization term. Defaults to 0.0001
+    X : {array-like, sparse matrix}, shape (n_samples, n_features)
+        Training data
+
+    y : numpy array, shape (n_samples,)
+        Target values
+
+    sample_weight : array-like, shape (n_samples,), optional
+        Weights applied to individual samples (1. for unweighted).
 
-    fit_intercept: bool, optional
-        Whether the intercept should be estimated or not. If False, the
-        data is assumed to be already centered. Defaults to True.
+    loss : 'log' | 'squared'
+        Loss function that will be optimized.
+        'log' is used for classification, like in LogisticRegression.
+        'squared' is used for regression, like in Ridge.
+
+    alpha : float, optional
+        Constant that multiplies the regularization term. Defaults to 1.
 
     max_iter: int, optional
         The max number of passes over the training data if the stopping
         criterea is not reached. Defaults to 1000.
 
     tol: double, optional
-        The stopping criterea for the weights. THe iterations will stop when
+        The stopping criterea for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
-    random_state: int or numpy.random.RandomState, optional
-        The random_state of the pseudo random number generator to use when
-        sampling the data.
-
     verbose: integer, optional
         The verbosity level.
 
-    eta0 : double or "auto"
-        The initial learning rate [default 0.01].
+    random_state : int seed, RandomState instance, or None (default)
+        The seed of the pseudo random number generator to use when
+        shuffling the data.
 
-    warm_start : bool, optional
-        When set to True, reuse the solution of the previous call to fit as
-        initialization, otherwise, just erase the previous solution.
+    check_input : bool, default True
+        If False, the input arrays X and y will not be checked.
 
-    Attributes
-    ----------
-    coef_ : array, shape (n_features,)
-        Weights asigned to the features.
+    max_squared_sum : float, default None
+        Maximum squared sum of X over samples. If None, it will be computed,
+        going through all the samples. The value should be precomputed
+        to speed up cross validation.
 
-    intercept_ : array, shape (1,)
-        The intercept term.
+    warm_start_mem: dict, optional
+        The initialization parameters used for warm starting. It is currently
+        not used in Ridge.
+
+    Returns
+    -------
+    coef_ : array, shape (n_features)
+        Weight vector.
+
+    n_iter_ : int
+        The number of full pass on all samples.
+
+    warm_start_mem : dict
+        Contains a 'coef' key with the fitted result, and eventually the
+        fitted intercept at the end of the array. Contains also other keys
+        used for warm starting.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
-    >>> y = np.random.randn(n_samples)
     >>> X = np.random.randn(n_samples, n_features)
-    >>> clf = linear_model.SAGRegressor()
+    >>> y = np.random.randn(n_samples)
+    >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
-    SAGRegressor(alpha=0.0001, eta0='auto',
-                 fit_intercept=True, max_iter=1000, random_state=None,
-                 tol=0.001, verbose=0, warm_start=False)
+    Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
+          normalize=False, random_state=None, solver='sag', tol=0.001)
+
+    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
+    >>> y = np.array([1, 1, 2, 2])
+    >>> clf = linear_model.LogisticRegression(solver='sag')
+    >>> clf.fit(X, y)
+    ... #doctest: +NORMALIZE_WHITESPACE
+    LogisticRegression(C=1.0, class_weight=None, dual=False,
+        fit_intercept=True, intercept_scaling=1, max_iter=100,
+        multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
+        solver='sag', tol=0.0001, verbose=0, warm_start=False)
+
+    Reference
+    ---------
+    Schmidt, M., Roux, N. L., & Bach, F. (2013).
+    Minimizing finite sums with the stochastic average gradient
+    https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
 
     See also
     --------
-    SGDRegressor, Ridge, ElasticNet, Lasso, SVR
-
+    Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
+    LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
-    def __init__(self, alpha=0.0001, fit_intercept=True, max_iter=1000,
-                 tol=0.001, verbose=0, random_state=None, eta0='auto',
-                 warm_start=False):
-
-        self.loss_function = SquaredLoss()
-        super(SAGRegressor, self).__init__(alpha=alpha,
-                                           fit_intercept=fit_intercept,
-                                           max_iter=max_iter,
-                                           verbose=verbose,
-                                           random_state=random_state,
-                                           tol=tol,
-                                           eta0=eta0,
-                                           warm_start=warm_start)
-
-    """Fit linear model with Stochastic Average Gradient.
-
-        Parameters
-        ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Training data
-
-        y : numpy array, shape (n_samples,)
-            Target values
-
-        sample_weight : array-like, shape (n_samples,), optional
-            Weights applied to individual samples (1. for unweighted).
-
-        Returns
-        -------
-        self : returns an instance of self.
-        """
-    def fit(self, X, y, sample_weight=None):
-        X, y = check_X_y(X, y, "csr", copy=False, order='C', dtype=np.float64)
-        y = y.astype(np.float64)
-
-        coef_init = None
-        intercept_init = None
-        sum_gradient_init = None
-        gradient_memory_init = None
-        seen_init = None
-        num_seen_init = None
-        intercept_sum_gradient_init = None
-
-        if self.warm_start:
-            coef_init = self.coef_
-            intercept_init = self.intercept_
-            sum_gradient_init = self.sum_gradient_
-            gradient_memory_init = self.gradient_memory_
-            seen_init = self.seen_
-            num_seen_init = self.num_seen_
-            intercept_sum_gradient_init = self.intercept_sum_gradient_
-
-        (self.coef_, self.intercept_, self.sum_gradient_,
-         self.gradient_memory_, self.seen_, self.num_seen_,
-         self.intercept_sum_gradient_) = \
-            super(SAGRegressor, self)._fit(X, y, coef_init,
-                                           intercept_init,
-                                           sample_weight,
-                                           sum_gradient_init,
-                                           gradient_memory_init,
-                                           seen_init, num_seen_init,
-                                           intercept_sum_gradient_init)
-
-        return self
+    # Ridge default max_iter is None
+    if max_iter is None:
+        max_iter = 1000
+
+    if check_input:
+        X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
+        y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
+
+    n_samples, n_features = X.shape[0], X.shape[1]
+    # As in SGD, the alpha is scaled by n_samples.
+    alpha_scaled = float(alpha) / n_samples
+
+    # initialization
+    if sample_weight is None:
+        sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
+
+    if 'coef' in warm_start_mem.keys():
+        coef_init = warm_start_mem['coef']
+    else:
+        coef_init = np.zeros(n_features, dtype=np.float64, order='C')
+
+    # coef_init contains possibly the intercept_init at the end.
+    # Note that Ridge centers the data before fitting, so fit_intercept=False.
+    fit_intercept = coef_init.size == (n_features + 1)
+    if fit_intercept:
+        intercept_init = coef_init[-1]
+        coef_init = coef_init[:-1]
+    else:
+        intercept_init = 0.0
+
+    if 'intercept_sum_gradient' in warm_start_mem.keys():
+        intercept_sum_gradient_init = warm_start_mem['intercept_sum_gradient']
+    else:
+        intercept_sum_gradient_init = 0.0
+
+    if 'gradient_memory' in warm_start_mem.keys():
+        gradient_memory_init = warm_start_mem['gradient_memory']
+    else:
+        gradient_memory_init = np.zeros(n_samples, dtype=np.float64,
+                                        order='C')
+    if 'sum_gradient' in warm_start_mem.keys():
+        sum_gradient_init = warm_start_mem['sum_gradient']
+    else:
+        sum_gradient_init = np.zeros(n_features, dtype=np.float64, order='C')
+
+    if 'seen' in warm_start_mem.keys():
+        seen_init = warm_start_mem['seen']
+    else:
+        seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
+
+    if 'num_seen' in warm_start_mem.keys():
+        num_seen_init = warm_start_mem['num_seen']
+    else:
+        num_seen_init = 0
+
+    dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
+
+    if max_squared_sum is None:
+        max_squared_sum = get_max_squared_sum(X)
+    step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
+                                   fit_intercept)
+
+    if step_size * alpha_scaled == 1:
+        raise ZeroDivisionError("Current sag implementation does not handle "
+                                "the case step_size * alpha_scaled == 1")
+
+    if loss == 'log':
+        class_loss = Log()
+    elif loss == 'squared':
+        class_loss = SquaredLoss()
+    else:
+        raise ValueError("Invalid loss parameter: got %r instead of "
+                         "one of ('log', 'squared')" % loss)
+
+    intercept_, num_seen, n_iter_, intercept_sum_gradient = \
+        sag(dataset, coef_init.ravel(),
+            intercept_init, n_samples,
+            n_features, tol,
+            max_iter,
+            class_loss,
+            step_size, alpha_scaled,
+            sum_gradient_init.ravel(),
+            gradient_memory_init.ravel(),
+            seen_init.ravel(),
+            num_seen_init,
+            fit_intercept,
+            intercept_sum_gradient_init,
+            intercept_decay,
+            verbose)
+
+    if n_iter_ == max_iter:
+        warnings.warn("The max_iter was reached which means "
+                      "the coef_ did not converge", ConvergenceWarning)
+
+    coef_ = coef_init
+    if fit_intercept:
+        coef_ = np.append(coef_, intercept_)
+
+    warm_start_mem = {'coef': coef_, 'sum_gradient': sum_gradient_init,
+                      'intercept_sum_gradient': intercept_sum_gradient,
+                      'gradient_memory': gradient_memory_init,
+                      'seen': seen_init, 'num_seen': num_seen}
+
+    return coef_, n_iter_, warm_start_mem

commit 4ceffe05a1bd5f63433f02cfd02af036f6026806
Author: Danny Sullivan <dsullivan7@hotmail.com>
Date:   Thu Oct 30 12:00:11 2014 +0100

    Adding Implementation of SAG

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
--- /dev/null
+++ b/sklearn/linear_model/sag.py
@@ -0,0 +251,310 @@
+    def __init__(self, alpha=0.0001,
+                 fit_intercept=True, max_iter=1000, tol=0.001, verbose=0,
+                 n_jobs=1, random_state=None,
+                 eta0='auto', class_weight=None, warm_start=False):
+        self.n_jobs = n_jobs
+        self.class_weight = class_weight
+        self.loss_function = Log()
+        super(SAGClassifier, self).__init__(alpha=alpha,
+                                            fit_intercept=fit_intercept,
+                                            max_iter=max_iter,
+                                            verbose=verbose,
+                                            random_state=random_state,
+                                            tol=tol,
+                                            eta0=eta0,
+                                            warm_start=warm_start)
+
+    """Fit linear model with Stochastic Average Gradient.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Training data
+
+        y : numpy array, shape (n_samples,)
+            Target values
+
+        sample_weight : array-like, shape (n_samples,), optional
+            Weights applied to individual samples (1. for unweighted).
+
+        Returns
+        -------
+        self : returns an instance of self.
+        """
+    def fit(self, X, y, sample_weight=None):
+        X, y = check_X_y(X, y, "csr", copy=False, order='C',
+                         dtype=np.float64)
+        n_samples, n_features = X.shape[0], X.shape[1]
+
+        self.classes_ = np.unique(y)
+        self.expanded_class_weight_ = compute_class_weight(self.class_weight,
+                                                           self.classes_, y)
+
+        if self.classes_.shape[0] <= 1:
+            # there is only one class
+            raise ValueError("The number of class labels must be "
+                             "greater than one.")
+        elif self.classes_.shape[0] == 2:
+            # binary classifier
+            (coef, intercept, sum_gradient, gradient_memory,
+             seen, num_seen, intercept_sum_gradient) = \
+                self._fit_target_class(X, y, self.classes_[1], sample_weight)
+        else:
+            # multiclass classifier
+            coef = []
+            intercept = []
+            sum_gradient = []
+            gradient_memory = []
+            seen = []
+            num_seen = []
+            intercept_sum_gradient = []
+
+            # perform a fit for all classes, one verse all
+            results = Parallel(n_jobs=self.n_jobs,
+                               backend="threading",
+                               verbose=self.verbose)(
+                # we have to use a call to multiprocess_method instead of the
+                # plain instance method because pickle will not work on
+                # instance methods in python 2.6 and 2.7
+                delayed(multiprocess_method)(self, "_fit_target_class",
+                                             (X, y, cl, sample_weight))
+                for cl in self.classes_)
+
+            # append results to the correct array
+            for (coef_cl, intercept_cl, sum_gradient_cl, gradient_memory_cl,
+                 seen_cl, num_seen_cl, intercept_sum_gradient_cl) in results:
+                coef.append(coef_cl)
+                intercept.append(intercept_cl)
+                sum_gradient.append(sum_gradient_cl)
+                gradient_memory.append(gradient_memory_cl)
+                seen.append(seen_cl)
+                num_seen.append(num_seen_cl)
+                intercept_sum_gradient.append(intercept_sum_gradient_cl)
+
+            # stack all arrays to transform into np arrays
+            coef = np.vstack(coef)
+            intercept = np.array(intercept)
+            sum_gradient = np.vstack(sum_gradient)
+            gradient_memory = np.vstack(gradient_memory)
+            seen = np.vstack(seen)
+            num_seen = np.array(num_seen)
+            intercept_sum_gradient = np.array(intercept_sum_gradient)
+
+        self.coef_ = coef
+        self.intercept_ = intercept
+        self.sum_gradient_ = sum_gradient
+        self.gradient_memory_ = gradient_memory
+        self.seen_ = seen
+        self.num_seen_ = num_seen
+        self.intercept_sum_gradient_ = intercept_sum_gradient
+
+        return self
+
+    def _fit_target_class(self, X, y, target_class, sample_weight=None):
+        coef_init = None
+        intercept_init = None
+        sum_gradient_init = None
+        gradient_memory_init = None
+        seen_init = None
+        num_seen_init = None
+        intercept_sum_gradient_init = None
+
+        if self.classes_.shape[0] == 2:
+            if self.warm_start:
+                # init parameters for binary classifier
+                coef_init = self.coef_
+                intercept_init = self.intercept_
+                sum_gradient_init = self.sum_gradient_
+                gradient_memory_init = self.gradient_memory_
+                seen_init = self.seen_
+                num_seen_init = self.num_seen_
+                intercept_sum_gradient_init = \
+                    self.intercept_sum_gradient_
+
+            weight_pos = self.expanded_class_weight_[1]
+            weight_neg = self.expanded_class_weight_[0]
+        else:
+            class_index = np.where(self.classes_ == target_class)[0][0]
+            if self.warm_start:
+                # init parameters for multi-class classifier
+                if self.coef_ is not None:
+                    coef_init = self.coef_[class_index]
+                if self.intercept_ is not None:
+                    intercept_init = self.intercept_[class_index]
+                if self.sum_gradient_ is not None:
+                    sum_gradient_init = self.sum_gradient_[class_index]
+                if self.gradient_memory_ is not None:
+                    gradient_memory_init = self.gradient_memory_[class_index]
+                if self.seen_ is not None:
+                    seen_init = self.seen_[class_index]
+                if self.num_seen_ is not None:
+                    num_seen_init = self.num_seen_[class_index]
+                if self.intercept_sum_gradient_ is not None:
+                    intercept_sum_gradient_init = \
+                        self.intercept_sum_gradient_[class_index]
+
+            weight_pos = self.expanded_class_weight_[class_index]
+            weight_neg = 1.0
+
+        n_samples, n_features = X.shape[0], X.shape[1]
+
+        y_encoded = np.ones(n_samples)
+        y_encoded[y != target_class] = -1.0
+
+        return super(SAGClassifier, self).\
+            _fit(X, y_encoded,
+                 coef_init, intercept_init,
+                 sample_weight,
+                 sum_gradient_init,
+                 gradient_memory_init,
+                 seen_init, num_seen_init,
+                 intercept_sum_gradient_init,
+                 weight_pos, weight_neg)
+
+
+class SAGRegressor(BaseSAG, LinearModel, RegressorMixin,
+                   BaseEstimator):
+    """Linear model fitted by minimizing a regularized empirical loss with SAG
+
+    SAG stands for Stochastic Average Gradient: the gradient of the loss is
+    estimated each sample at a time and the model is updated along the way with
+    a constant learning rate. The inspiration for SAG comes from "Minimizing
+    Finite Sums with the Stochastic Average Gradient" by Mark Schmidt,
+    Nicolas Le Roux, and Francis Bach. 2013. <hal-00860051>
+    https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
+
+    IMPORTANT NOTE: SAGRegressor and models from linear_model in general depend
+    on columns that are on the same scale. You can make sure that the data will
+    be normalized by using sklearn.preprocessing.StandardScaler on your data
+    before passing it to the fit method.
+
+    The regularizer is a penalty added to the loss function that shrinks model
+    parameters towards the zero vector using the squared euclidean norm
+    L2.
+
+    This implementation works with data represented as dense or sparse numpy
+    arrays of floating point values for the features.
+
+    Parameters
+    ----------
+    alpha : float, optional
+        Constant that multiplies the regularization term. Defaults to 0.0001
+
+    fit_intercept: bool, optional
+        Whether the intercept should be estimated or not. If False, the
+        data is assumed to be already centered. Defaults to True.
+
+    max_iter: int, optional
+        The max number of passes over the training data if the stopping
+        criterea is not reached. Defaults to 1000.
+
+    tol: double, optional
+        The stopping criterea for the weights. THe iterations will stop when
+        max(change in weights) / max(weights) < tol. Defaults to .001
+
+    random_state: int or numpy.random.RandomState, optional
+        The random_state of the pseudo random number generator to use when
+        sampling the data.
+
+    verbose: integer, optional
+        The verbosity level.
+
+    eta0 : double or "auto"
+        The initial learning rate [default 0.01].
+
+    warm_start : bool, optional
+        When set to True, reuse the solution of the previous call to fit as
+        initialization, otherwise, just erase the previous solution.
+
+    Attributes
+    ----------
+    coef_ : array, shape (n_features,)
+        Weights asigned to the features.
+
+    intercept_ : array, shape (1,)
+        The intercept term.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn import linear_model
+    >>> n_samples, n_features = 10, 5
+    >>> np.random.seed(0)
+    >>> y = np.random.randn(n_samples)
+    >>> X = np.random.randn(n_samples, n_features)
+    >>> clf = linear_model.SAGRegressor()
+    >>> clf.fit(X, y)
+    ... #doctest: +NORMALIZE_WHITESPACE
+    SAGRegressor(alpha=0.0001, eta0='auto',
+                 fit_intercept=True, max_iter=1000, random_state=None,
+                 tol=0.001, verbose=0, warm_start=False)
+
+    See also
+    --------
+    SGDRegressor, Ridge, ElasticNet, Lasso, SVR
+
+    """
+    def __init__(self, alpha=0.0001, fit_intercept=True, max_iter=1000,
+                 tol=0.001, verbose=0, random_state=None, eta0='auto',
+                 warm_start=False):
+
+        self.loss_function = SquaredLoss()
+        super(SAGRegressor, self).__init__(alpha=alpha,
+                                           fit_intercept=fit_intercept,
+                                           max_iter=max_iter,
+                                           verbose=verbose,
+                                           random_state=random_state,
+                                           tol=tol,
+                                           eta0=eta0,
+                                           warm_start=warm_start)
+
+    """Fit linear model with Stochastic Average Gradient.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Training data
+
+        y : numpy array, shape (n_samples,)
+            Target values
+
+        sample_weight : array-like, shape (n_samples,), optional
+            Weights applied to individual samples (1. for unweighted).
+
+        Returns
+        -------
+        self : returns an instance of self.
+        """
+    def fit(self, X, y, sample_weight=None):
+        X, y = check_X_y(X, y, "csr", copy=False, order='C', dtype=np.float64)
+        y = y.astype(np.float64)
+
+        coef_init = None
+        intercept_init = None
+        sum_gradient_init = None
+        gradient_memory_init = None
+        seen_init = None
+        num_seen_init = None
+        intercept_sum_gradient_init = None
+
+        if self.warm_start:
+            coef_init = self.coef_
+            intercept_init = self.intercept_
+            sum_gradient_init = self.sum_gradient_
+            gradient_memory_init = self.gradient_memory_
+            seen_init = self.seen_
+            num_seen_init = self.num_seen_
+            intercept_sum_gradient_init = self.intercept_sum_gradient_
+
+        (self.coef_, self.intercept_, self.sum_gradient_,
+         self.gradient_memory_, self.seen_, self.num_seen_,
+         self.intercept_sum_gradient_) = \
+            super(SAGRegressor, self)._fit(X, y, coef_init,
+                                           intercept_init,
+                                           sample_weight,
+                                           sum_gradient_init,
+                                           gradient_memory_init,
+                                           seen_init, num_seen_init,
+                                           intercept_sum_gradient_init)
+
+        return self
