commit 94eb61960a13f84a8be0dd527012f929172a0d60
Author: TomDLT <tom.dupre-la-tour@m4x.org>
Date:   Tue May 19 10:24:22 2015 +0200

    ENH add sag solver in LogisticRegression and Ridge

diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
index 71eddbe00..0d6cfe03e 100644
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -1,478 +1,139 @@
-import numpy as np
-import scipy.sparse as sp
+"""Solvers for Ridge and LogisticRegression using SAG algorithm"""
+
+# Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>
+#
+# Licence: BSD 3 clause
 
-from abc import ABCMeta
+import numpy as np
 import warnings
 
-from .base import LinearClassifierMixin, LinearModel, SparseCoefMixin
-from ..base import RegressorMixin, BaseEstimator
-from ..utils import check_X_y, compute_class_weight, check_random_state
 from ..utils import ConvergenceWarning
-from ..utils.seq_dataset import ArrayDataset, CSRDataset
-from ..externals import six
-from ..externals.joblib import Parallel, delayed
-from .sag_fast import Log, SquaredLoss
-from .sag_fast import sag_sparse, get_auto_eta
-
-MAX_INT = np.iinfo(np.int32).max
-
-"""For sparse data intercept updates are scaled by this decay factor to avoid
-intercept oscillation."""
-SPARSE_INTERCEPT_DECAY = 0.01
-
-
-# taken from http://stackoverflow.com/questions/1816958
-# useful for passing instance methods to Parallel
-def multiprocess_method(instance, name, args=()):
-    "indirect caller for instance methods and multiprocessing"
-    return getattr(instance, name)(*args)
+from ..utils import check_array
+from .base import make_dataset
+from .sgd_fast import Log, SquaredLoss
+from .sag_fast import sag, get_max_squared_sum
 
 
-# The inspiration for SAG comes from:
-# "Minimizing Finite Sums with the Stochastic Average Gradient" by
-# Mark Schmidt, Nicolas Le Roux, Francis Bach. 2013. <hal-00860051>
-#
-# https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
-class BaseSAG(six.with_metaclass(ABCMeta, SparseCoefMixin)):
-    def __init__(self, alpha=0.0001, fit_intercept=True, max_iter=1000,
-                 tol=0.001, verbose=0,
-                 random_state=None, eta0='auto', warm_start=False):
-        self.alpha = alpha
-        self.fit_intercept = fit_intercept
-        self.max_iter = max_iter
-        self.tol = tol
-        self.verbose = verbose
-        self.eta0 = eta0
-        self.random_state = random_state
-        self.warm_start = warm_start
-
-        self._validate_params()
-
-        self.coef_ = None
-        self.intercept_ = None
-
-        self.num_seen_ = None
-        self.seen_ = None
-        self.sum_gradient_ = None
-        self.gradient_memory_ = None
-        self.intercept_sum_gradient_ = None
-
-    def _validate_params(self):
-        if not isinstance(self.max_iter, int):
-            raise ValueError("max_iter must be an integer")
-        if self.max_iter < 1:
-            raise ValueError("max_iter must be greater than 0")
-
-    def _fit(self, X, y, coef_init=None, intercept_init=None,
-             sample_weight=None, sum_gradient_init=None,
-             gradient_memory_init=None, seen_init=None, num_seen_init=None,
-             intercept_sum_gradient_init=None,
-             weight_pos=1.0, weight_neg=1.0):
-
-        n_samples, n_features = X.shape[0], X.shape[1]
-
-        # initialize all parameters if there is no init
-        if sample_weight is None:
-            sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
-
-        if intercept_init is None:
-            intercept_init = 0.0
-
-        if intercept_sum_gradient_init is None:
-            intercept_sum_gradient_init = 0.0
-
-        if coef_init is None:
-            coef_init = np.zeros(n_features, dtype=np.float64, order='C')
-
-        if sum_gradient_init is None:
-            sum_gradient_init = np.zeros(n_features, dtype=np.float64,
-                                         order='C')
-
-        if gradient_memory_init is None:
-            gradient_memory_init = np.zeros(n_samples, dtype=np.float64,
-                                            order='C')
-
-        if seen_init is None:
-            seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
-
-        if num_seen_init is None:
-            num_seen_init = 0
-
-        random_state = check_random_state(self.random_state)
-
-        # check which type of Sequential Dataset is needed
-        if sp.issparse(X):
-            dataset = CSRDataset(X.data, X.indptr, X.indices,
-                                 y, sample_weight,
-                                 seed=random_state.randint(MAX_INT))
-            intercept_decay = SPARSE_INTERCEPT_DECAY
-        else:
-            dataset = ArrayDataset(X, y, sample_weight,
-                                   seed=random_state.randint(MAX_INT))
-            intercept_decay = 1.0
-
-        # set the eta0 if needed, 'auto' is 1 / 4L where L is the max sum of
-        # squares for over all samples
-        if self.eta0 == 'auto':
-            step_size = get_auto_eta(dataset, self.alpha, n_samples,
-                                     self.loss_function, self.fit_intercept)
-        else:
-            step_size = self.eta0
-
-        intercept_, num_seen, max_iter_reached, intercept_sum_gradient = \
-            sag_sparse(dataset, coef_init.ravel(),
-                       intercept_init, n_samples,
-                       n_features, self.tol,
-                       self.max_iter,
-                       self.loss_function,
-                       step_size, self.alpha,
-                       sum_gradient_init.ravel(),
-                       gradient_memory_init.ravel(),
-                       seen_init.ravel(),
-                       num_seen_init, weight_pos,
-                       weight_neg,
-                       self.fit_intercept,
-                       intercept_sum_gradient_init,
-                       intercept_decay,
-                       self.verbose)
-
-        if max_iter_reached:
-            warnings.warn("The max_iter was reached which means "
-                          "the coef_ did not converge", ConvergenceWarning)
-
-        return (coef_init.reshape(1, -1), intercept_,
-                sum_gradient_init.reshape(1, -1),
-                gradient_memory_init.reshape(1, -1),
-                seen_init.reshape(1, -1),
-                num_seen, intercept_sum_gradient)
-
-
-class SAGClassifier(BaseSAG, LinearClassifierMixin, BaseEstimator):
-    """Linear classifiers (SVM, logistic regression, a.o.) with SAG training.
-
-    This estimator implements regularized linear models with stochastic
-    average gradient (SAG) learning: the gradient of the loss is estimated
-    using a random sample from the dataset. The weights are then updated
-    according to the sum of gradients seen thus far divided by the number of
-    unique samples seen. The inspiration for SAG comes from "Minimizing Finite
-    Sums with the Stochastic Average Gradient" by Mark Schmidt, Nicolas Le
-    Roux, and Francis Bach. 2013. <hal-00860051>
-    https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
-
-    IMPORTANT NOTE: SAGClassifier and models from linear_model in general
-    depend on columns that are on the same scale. You can make sure that the
-    data will be normalized by using sklearn.preprocessing.StandardScaler on
-    your data before passing it to the fit method.
-
-    This implementation works with data represented as dense or sparse arrays
-    of floating point values for the features. It will fit the data according
-    to log loss.
+def get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept):
+    """Compute automatic step size for SAG solver
 
-    The regularizer is a penalty added to the loss function that shrinks model
-    parameters towards the zero vector using either the squared euclidean norm
-    L2.
+    The step size is set to 1 / (alpha_scaled + L + fit_intercept) where L is
+    the max sum of squares for over all samples.
 
     Parameters
     ----------
-    alpha : float, optional
-        Constant that multiplies the regularization term. Defaults to 0.0001
-
-    fit_intercept: bool, optional
-        Whether the intercept should be estimated or not. If False, the
-        data is assumed to be already centered. Defaults to True.
-
-    max_iter: int, optional
-        The max number of passes over the training data if the stopping
-        criterea is not reached. Defaults to 1000.
-
-    tol: double, optional
-        The stopping criterea for the weights. THe iterations will stop when
-        max(change in weights) / max(weights) < tol. Defaults to .001
-
-    random_state: int or numpy.random.RandomState, optional
-        The random_state of the pseudo random number generator to use when
-        sampling the data.
-
-    verbose: integer, optional
-        The verbosity level
-
-    n_jobs: integer, optional
-        The number of CPUs to use to do the OVA (One Versus All, for
-        multi-class problems) computation. -1 means 'all CPUs'. Defaults
-        to 1.
-
-    eta0 : double or "auto"
-        The initial learning rate. The default value is 0.001.
-
-    class_weight : dict, {class_label : weight} or "auto" or None, optional
-        Preset for the class_weight fit parameter.
-
-        Weights associated with classes. If not given, all classes
-        are supposed to have weight one.
-
-        The "auto" mode uses the values of y to automatically adjust
-        weights inversely proportional to class frequencies.
-
-    warm_start : bool, optional
-        When set to True, reuse the solution of the previous call to fit as
-        initialization, otherwise, just erase the previous solution.
+    max_squared_sum : float
+        Maximum squared sum of X over samples.
 
+    alpha_scaled : float
+        Constant that multiplies the regularization term, scaled by
+        1. / n_samples, the number of samples.
 
-    Attributes
-    ----------
-    coef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes,
-    n_features)
-        Weights assigned to the features.
-
-    intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,)
-        Constants in decision function.
+    loss : string, in {"log", "squared"}
+        The loss function used in SAG solver.
 
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from sklearn import linear_model
-    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
-    >>> Y = np.array([1, 1, 2, 2])
-    >>> clf = linear_model.SAGClassifier()
-    >>> clf.fit(X, Y)
-    ... #doctest: +NORMALIZE_WHITESPACE
-    SAGClassifier(alpha=0.0001, class_weight=None,
-                  eta0='auto', fit_intercept=True,
-                  max_iter=1000, n_jobs=1, random_state=None,
-                  tol=0.001, verbose=0, warm_start=False)
-    >>> print(clf.predict([[-0.8, -1]]))
-    [1]
+    fit_intercept : bool
+        Specifies if a constant (a.k.a. bias or intercept) will be
+        added to the decision function.
 
-    See also
-    --------
-    SGDClassifier, LinearSVC, LogisticRegression, Perceptron
+    Returns
+    -------
+    step_size : float
+        Step size used in SAG solver.
 
     """
-    def __init__(self, alpha=0.0001,
-                 fit_intercept=True, max_iter=1000, tol=0.001, verbose=0,
-                 n_jobs=1, random_state=None,
-                 eta0='auto', class_weight=None, warm_start=False):
-        self.n_jobs = n_jobs
-        self.class_weight = class_weight
-        self.loss_function = Log()
-        super(SAGClassifier, self).__init__(alpha=alpha,
-                                            fit_intercept=fit_intercept,
-                                            max_iter=max_iter,
-                                            verbose=verbose,
-                                            random_state=random_state,
-                                            tol=tol,
-                                            eta0=eta0,
-                                            warm_start=warm_start)
-
-    """Fit linear model with Stochastic Average Gradient.
-
-        Parameters
-        ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Training data
-
-        y : numpy array, shape (n_samples,)
-            Target values
-
-        sample_weight : array-like, shape (n_samples,), optional
-            Weights applied to individual samples (1. for unweighted).
-
-        Returns
-        -------
-        self : returns an instance of self.
-        """
-    def fit(self, X, y, sample_weight=None):
-        X, y = check_X_y(X, y, "csr", copy=False, order='C',
-                         dtype=np.float64)
-        n_samples, n_features = X.shape[0], X.shape[1]
-
-        self.classes_ = np.unique(y)
-        self.expanded_class_weight_ = compute_class_weight(self.class_weight,
-                                                           self.classes_, y)
-
-        if self.classes_.shape[0] <= 1:
-            # there is only one class
-            raise ValueError("The number of class labels must be "
-                             "greater than one.")
-        elif self.classes_.shape[0] == 2:
-            # binary classifier
-            (coef, intercept, sum_gradient, gradient_memory,
-             seen, num_seen, intercept_sum_gradient) = \
-                self._fit_target_class(X, y, self.classes_[1], sample_weight)
-        else:
-            # multiclass classifier
-            coef = []
-            intercept = []
-            sum_gradient = []
-            gradient_memory = []
-            seen = []
-            num_seen = []
-            intercept_sum_gradient = []
-
-            # perform a fit for all classes, one verse all
-            results = Parallel(n_jobs=self.n_jobs,
-                               backend="threading",
-                               verbose=self.verbose)(
-                # we have to use a call to multiprocess_method instead of the
-                # plain instance method because pickle will not work on
-                # instance methods in python 2.6 and 2.7
-                delayed(multiprocess_method)(self, "_fit_target_class",
-                                             (X, y, cl, sample_weight))
-                for cl in self.classes_)
-
-            # append results to the correct array
-            for (coef_cl, intercept_cl, sum_gradient_cl, gradient_memory_cl,
-                 seen_cl, num_seen_cl, intercept_sum_gradient_cl) in results:
-                coef.append(coef_cl)
-                intercept.append(intercept_cl)
-                sum_gradient.append(sum_gradient_cl)
-                gradient_memory.append(gradient_memory_cl)
-                seen.append(seen_cl)
-                num_seen.append(num_seen_cl)
-                intercept_sum_gradient.append(intercept_sum_gradient_cl)
-
-            # stack all arrays to transform into np arrays
-            coef = np.vstack(coef)
-            intercept = np.array(intercept)
-            sum_gradient = np.vstack(sum_gradient)
-            gradient_memory = np.vstack(gradient_memory)
-            seen = np.vstack(seen)
-            num_seen = np.array(num_seen)
-            intercept_sum_gradient = np.array(intercept_sum_gradient)
-
-        self.coef_ = coef
-        self.intercept_ = intercept
-        self.sum_gradient_ = sum_gradient
-        self.gradient_memory_ = gradient_memory
-        self.seen_ = seen
-        self.num_seen_ = num_seen
-        self.intercept_sum_gradient_ = intercept_sum_gradient
-
-        return self
-
-    def _fit_target_class(self, X, y, target_class, sample_weight=None):
-        coef_init = None
-        intercept_init = None
-        sum_gradient_init = None
-        gradient_memory_init = None
-        seen_init = None
-        num_seen_init = None
-        intercept_sum_gradient_init = None
-
-        if self.classes_.shape[0] == 2:
-            if self.warm_start:
-                # init parameters for binary classifier
-                coef_init = self.coef_
-                intercept_init = self.intercept_
-                sum_gradient_init = self.sum_gradient_
-                gradient_memory_init = self.gradient_memory_
-                seen_init = self.seen_
-                num_seen_init = self.num_seen_
-                intercept_sum_gradient_init = \
-                    self.intercept_sum_gradient_
-
-            weight_pos = self.expanded_class_weight_[1]
-            weight_neg = self.expanded_class_weight_[0]
-        else:
-            class_index = np.where(self.classes_ == target_class)[0][0]
-            if self.warm_start:
-                # init parameters for multi-class classifier
-                if self.coef_ is not None:
-                    coef_init = self.coef_[class_index]
-                if self.intercept_ is not None:
-                    intercept_init = self.intercept_[class_index]
-                if self.sum_gradient_ is not None:
-                    sum_gradient_init = self.sum_gradient_[class_index]
-                if self.gradient_memory_ is not None:
-                    gradient_memory_init = self.gradient_memory_[class_index]
-                if self.seen_ is not None:
-                    seen_init = self.seen_[class_index]
-                if self.num_seen_ is not None:
-                    num_seen_init = self.num_seen_[class_index]
-                if self.intercept_sum_gradient_ is not None:
-                    intercept_sum_gradient_init = \
-                        self.intercept_sum_gradient_[class_index]
-
-            weight_pos = self.expanded_class_weight_[class_index]
-            weight_neg = 1.0
-
-        n_samples, n_features = X.shape[0], X.shape[1]
-
-        y_encoded = np.ones(n_samples)
-        y_encoded[y != target_class] = -1.0
-
-        return super(SAGClassifier, self).\
-            _fit(X, y_encoded,
-                 coef_init, intercept_init,
-                 sample_weight,
-                 sum_gradient_init,
-                 gradient_memory_init,
-                 seen_init, num_seen_init,
-                 intercept_sum_gradient_init,
-                 weight_pos, weight_neg)
-
-
-class SAGRegressor(BaseSAG, LinearModel, RegressorMixin,
-                   BaseEstimator):
-    """Linear model fitted by minimizing a regularized empirical loss with SAG
+    if loss == 'log':
+        # inverse Lipschitz constant for log loss
+        return 4.0 / (max_squared_sum + int(fit_intercept)
+                      + 4.0 * alpha_scaled)
+    elif loss == 'squared':
+        # inverse Lipschitz constant for squared loss
+        return 1.0 / (max_squared_sum + int(fit_intercept) + alpha_scaled)
+    else:
+        raise ValueError("Unknown loss function for SAG solver, got %s "
+                         "instead of 'log' or 'squared'" % loss)
+
+
+def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
+               max_iter=1000, tol=0.001, verbose=0, random_state=None,
+               check_input=True, max_squared_sum=None,
+               warm_start_mem=dict()):
+    """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
     estimated each sample at a time and the model is updated along the way with
-    a constant learning rate. The inspiration for SAG comes from "Minimizing
-    Finite Sums with the Stochastic Average Gradient" by Mark Schmidt,
-    Nicolas Le Roux, and Francis Bach. 2013. <hal-00860051>
-    https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
+    a constant learning rate.
 
-    IMPORTANT NOTE: SAGRegressor and models from linear_model in general depend
-    on columns that are on the same scale. You can make sure that the data will
-    be normalized by using sklearn.preprocessing.StandardScaler on your data
-    before passing it to the fit method.
+    IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
+    same scale. You can normalize the data by using
+    sklearn.preprocessing.StandardScaler on your data before passing it to the
+    fit method.
 
-    The regularizer is a penalty added to the loss function that shrinks model
-    parameters towards the zero vector using the squared euclidean norm
-    L2.
+    This implementation works with data represented as dense numpy arrays or
+    sparse scipy arrays of floating point values for the features. It will
+    fit the data according to squared loss or log loss.
 
-    This implementation works with data represented as dense or sparse numpy
-    arrays of floating point values for the features.
+    The regularizer is a penalty added to the loss function that shrinks model
+    parameters towards the zero vector using the squared euclidean norm L2.
 
     Parameters
     ----------
-    alpha : float, optional
-        Constant that multiplies the regularization term. Defaults to 0.0001
+    X : {array-like, sparse matrix}, shape (n_samples, n_features)
+        Training data
+
+    y : numpy array, shape (n_samples,)
+        Target values
+
+    sample_weight : array-like, shape (n_samples,), optional
+        Weights applied to individual samples (1. for unweighted).
 
-    fit_intercept: bool, optional
-        Whether the intercept should be estimated or not. If False, the
-        data is assumed to be already centered. Defaults to True.
+    loss : 'log' | 'squared'
+        Loss function that will be optimized.
+        'log' is used for classification, like in LogisticRegression.
+        'squared' is used for regression, like in Ridge.
+
+    alpha : float, optional
+        Constant that multiplies the regularization term. Defaults to 1.
 
     max_iter: int, optional
         The max number of passes over the training data if the stopping
         criterea is not reached. Defaults to 1000.
 
     tol: double, optional
-        The stopping criterea for the weights. THe iterations will stop when
+        The stopping criterea for the weights. The iterations will stop when
         max(change in weights) / max(weights) < tol. Defaults to .001
 
-    random_state: int or numpy.random.RandomState, optional
-        The random_state of the pseudo random number generator to use when
-        sampling the data.
-
     verbose: integer, optional
         The verbosity level.
 
-    eta0 : double or "auto"
-        The initial learning rate [default 0.01].
+    random_state : int seed, RandomState instance, or None (default)
+        The seed of the pseudo random number generator to use when
+        shuffling the data.
 
-    warm_start : bool, optional
-        When set to True, reuse the solution of the previous call to fit as
-        initialization, otherwise, just erase the previous solution.
+    check_input : bool, default True
+        If False, the input arrays X and y will not be checked.
 
-    Attributes
-    ----------
-    coef_ : array, shape (n_features,)
-        Weights asigned to the features.
+    max_squared_sum : float, default None
+        Maximum squared sum of X over samples. If None, it will be computed,
+        going through all the samples. The value should be precomputed
+        to speed up cross validation.
 
-    intercept_ : array, shape (1,)
-        The intercept term.
+    warm_start_mem: dict, optional
+        The initialization parameters used for warm starting. It is currently
+        not used in Ridge.
+
+    Returns
+    -------
+    coef_ : array, shape (n_features)
+        Weight vector.
+
+    n_iter_ : int
+        The number of full pass on all samples.
+
+    warm_start_mem : dict
+        Contains a 'coef' key with the fitted result, and eventually the
+        fitted intercept at the end of the array. Contains also other keys
+        used for warm starting.
 
     Examples
     --------
@@ -480,81 +141,136 @@ class SAGRegressor(BaseSAG, LinearModel, RegressorMixin,
     >>> from sklearn import linear_model
     >>> n_samples, n_features = 10, 5
     >>> np.random.seed(0)
-    >>> y = np.random.randn(n_samples)
     >>> X = np.random.randn(n_samples, n_features)
-    >>> clf = linear_model.SAGRegressor()
+    >>> y = np.random.randn(n_samples)
+    >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
-    SAGRegressor(alpha=0.0001, eta0='auto',
-                 fit_intercept=True, max_iter=1000, random_state=None,
-                 tol=0.001, verbose=0, warm_start=False)
+    Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
+          normalize=False, random_state=None, solver='sag', tol=0.001)
+
+    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
+    >>> y = np.array([1, 1, 2, 2])
+    >>> clf = linear_model.LogisticRegression(solver='sag')
+    >>> clf.fit(X, y)
+    ... #doctest: +NORMALIZE_WHITESPACE
+    LogisticRegression(C=1.0, class_weight=None, dual=False,
+        fit_intercept=True, intercept_scaling=1, max_iter=100,
+        multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
+        solver='sag', tol=0.0001, verbose=0, warm_start=False)
+
+    Reference
+    ---------
+    Schmidt, M., Roux, N. L., & Bach, F. (2013).
+    Minimizing finite sums with the stochastic average gradient
+    https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
 
     See also
     --------
-    SGDRegressor, Ridge, ElasticNet, Lasso, SVR
-
+    Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
+    LogisticRegression, SGDClassifier, LinearSVC, Perceptron
     """
-    def __init__(self, alpha=0.0001, fit_intercept=True, max_iter=1000,
-                 tol=0.001, verbose=0, random_state=None, eta0='auto',
-                 warm_start=False):
-
-        self.loss_function = SquaredLoss()
-        super(SAGRegressor, self).__init__(alpha=alpha,
-                                           fit_intercept=fit_intercept,
-                                           max_iter=max_iter,
-                                           verbose=verbose,
-                                           random_state=random_state,
-                                           tol=tol,
-                                           eta0=eta0,
-                                           warm_start=warm_start)
-
-    """Fit linear model with Stochastic Average Gradient.
-
-        Parameters
-        ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Training data
-
-        y : numpy array, shape (n_samples,)
-            Target values
-
-        sample_weight : array-like, shape (n_samples,), optional
-            Weights applied to individual samples (1. for unweighted).
-
-        Returns
-        -------
-        self : returns an instance of self.
-        """
-    def fit(self, X, y, sample_weight=None):
-        X, y = check_X_y(X, y, "csr", copy=False, order='C', dtype=np.float64)
-        y = y.astype(np.float64)
-
-        coef_init = None
-        intercept_init = None
-        sum_gradient_init = None
-        gradient_memory_init = None
-        seen_init = None
-        num_seen_init = None
-        intercept_sum_gradient_init = None
-
-        if self.warm_start:
-            coef_init = self.coef_
-            intercept_init = self.intercept_
-            sum_gradient_init = self.sum_gradient_
-            gradient_memory_init = self.gradient_memory_
-            seen_init = self.seen_
-            num_seen_init = self.num_seen_
-            intercept_sum_gradient_init = self.intercept_sum_gradient_
-
-        (self.coef_, self.intercept_, self.sum_gradient_,
-         self.gradient_memory_, self.seen_, self.num_seen_,
-         self.intercept_sum_gradient_) = \
-            super(SAGRegressor, self)._fit(X, y, coef_init,
-                                           intercept_init,
-                                           sample_weight,
-                                           sum_gradient_init,
-                                           gradient_memory_init,
-                                           seen_init, num_seen_init,
-                                           intercept_sum_gradient_init)
-
-        return self
+    # Ridge default max_iter is None
+    if max_iter is None:
+        max_iter = 1000
+
+    if check_input:
+        X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
+        y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
+
+    n_samples, n_features = X.shape[0], X.shape[1]
+    # As in SGD, the alpha is scaled by n_samples.
+    alpha_scaled = float(alpha) / n_samples
+
+    # initialization
+    if sample_weight is None:
+        sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
+
+    if 'coef' in warm_start_mem.keys():
+        coef_init = warm_start_mem['coef']
+    else:
+        coef_init = np.zeros(n_features, dtype=np.float64, order='C')
+
+    # coef_init contains possibly the intercept_init at the end.
+    # Note that Ridge centers the data before fitting, so fit_intercept=False.
+    fit_intercept = coef_init.size == (n_features + 1)
+    if fit_intercept:
+        intercept_init = coef_init[-1]
+        coef_init = coef_init[:-1]
+    else:
+        intercept_init = 0.0
+
+    if 'intercept_sum_gradient' in warm_start_mem.keys():
+        intercept_sum_gradient_init = warm_start_mem['intercept_sum_gradient']
+    else:
+        intercept_sum_gradient_init = 0.0
+
+    if 'gradient_memory' in warm_start_mem.keys():
+        gradient_memory_init = warm_start_mem['gradient_memory']
+    else:
+        gradient_memory_init = np.zeros(n_samples, dtype=np.float64,
+                                        order='C')
+    if 'sum_gradient' in warm_start_mem.keys():
+        sum_gradient_init = warm_start_mem['sum_gradient']
+    else:
+        sum_gradient_init = np.zeros(n_features, dtype=np.float64, order='C')
+
+    if 'seen' in warm_start_mem.keys():
+        seen_init = warm_start_mem['seen']
+    else:
+        seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
+
+    if 'num_seen' in warm_start_mem.keys():
+        num_seen_init = warm_start_mem['num_seen']
+    else:
+        num_seen_init = 0
+
+    dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
+
+    if max_squared_sum is None:
+        max_squared_sum = get_max_squared_sum(X)
+    step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
+                                   fit_intercept)
+
+    if step_size * alpha_scaled == 1:
+        raise ZeroDivisionError("Current sag implementation does not handle "
+                                "the case step_size * alpha_scaled == 1")
+
+    if loss == 'log':
+        class_loss = Log()
+    elif loss == 'squared':
+        class_loss = SquaredLoss()
+    else:
+        raise ValueError("Invalid loss parameter: got %r instead of "
+                         "one of ('log', 'squared')" % loss)
+
+    intercept_, num_seen, n_iter_, intercept_sum_gradient = \
+        sag(dataset, coef_init.ravel(),
+            intercept_init, n_samples,
+            n_features, tol,
+            max_iter,
+            class_loss,
+            step_size, alpha_scaled,
+            sum_gradient_init.ravel(),
+            gradient_memory_init.ravel(),
+            seen_init.ravel(),
+            num_seen_init,
+            fit_intercept,
+            intercept_sum_gradient_init,
+            intercept_decay,
+            verbose)
+
+    if n_iter_ == max_iter:
+        warnings.warn("The max_iter was reached which means "
+                      "the coef_ did not converge", ConvergenceWarning)
+
+    coef_ = coef_init
+    if fit_intercept:
+        coef_ = np.append(coef_, intercept_)
+
+    warm_start_mem = {'coef': coef_, 'sum_gradient': sum_gradient_init,
+                      'intercept_sum_gradient': intercept_sum_gradient,
+                      'gradient_memory': gradient_memory_init,
+                      'seen': seen_init, 'num_seen': num_seen}
+
+    return coef_, n_iter_, warm_start_mem
